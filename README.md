# ğŸ§­ TESC: Deterministic Cognitive State Control in LLMs

[![arXiv](https://img.shields.io/badge/arXiv-Preprint-B31B1B.svg)](https://arxiv.org/)  
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

AMAWTA Research â€” Daslav RÃ­os Montecinos, Oscar RÃ­os Saldivar  
ğŸ“§ daslav@amawtalabs.com Â· oscar@amawtalabs.com  
ğŸŒ https://amawtalabs.com

---

## ğŸ“Œ Abstract
We present empirical validation of the Theorem of Semioticâ€“Cognitive Equivalence (TESC), demonstrating deterministic cognitive state control in LLMs through structured outputs and semiotic configuration.

Headline results (current lab + benches):
- JSON validity via vendor structured outputs (`response_schema`); incremental gains with TESC presets: coverage/severity = 1.00, stable behavior, and clear mode separability.
- Injectivity plateau 1.0 for thresholds â‰¥ 0.98
- Semiotic Uncertainty: Î”sÂ·Î”c â‰¥ â„_sem (â„_sem â‰ˆ 2Ã—10â»âµ) always satisfied
- Dynamics: deterministic fraction â‰ˆ 0.53 and SNR â‰ˆ 14 dB (linear, held-out)
- Benchmarks: programming pitfalls (review/repair) and audit contracts (ES)

---

## â© TL;DR for devs (utility first)

- We use vendor structured outputs (`response_schema`) as a contract. With semiotic presets (system instruction, style markers, bounded temperature), behavior is stable, modeâ€‘separable, and partly predictable â€” and contracts become machineâ€‘actionable (coverage/severity = 1.00) with external evaluation to avoid circularity.
- Same schema, consistent fields every time â†’ CIâ€‘ready tables and tests.
- Fewer flips under small wording/temperature changes (stability).
- Modes remain distinct (injectivity plateau).
- For functionâ€‘calling: complete arguments and nearâ€‘zero extra keys.

Scope note: JSON validity itself is provided by the modelâ€™s structured outputs; TESC focuses on the incremental gains above free text (coverage/completeness, stability/robustness, separability, and toolâ€‘calling quality).

### Minimal example

Schema (review JSON):
```json
{
  "type": "object",
  "properties": {
    "review": {
      "type": "object",
      "required": ["issues", "risks", "patch_outline", "tests", "severity"],
      "properties": {
        "issues":        {"type": "array", "items": {"type": "string"}},
        "risks":         {"type": "array", "items": {"type": "string"}},
        "patch_outline": {"type": "array", "items": {"type": "string"}},
        "tests":         {"type": "array", "items": {"type": "string"}},
        "severity":      {"type": "number"}
      }
    }
  },
  "required": ["review"]
}
```

System instruction (prompt snippet):
```
You are a senior code reviewer. Provide actionable findings, risks,
a patch outline, tests, and a numeric severity in [0,1].
Return JSON with fields: issues[], risks[], patch_outline[], tests[], severity.
```

Sample output:
```json
{
  "review": {
    "issues": ["Mutable default argument on parameter 'b'"],
    "risks": ["State leakage across calls"],
    "patch_outline": ["Use None and initialize list inside the function"],
    "tests": ["Call twice; lists must not alias"],
    "severity": 0.9
  }
}
```

### Two commands to reproduce (review bench)
```
poetry run python benchmarks/run_programming_bench.py
poetry run python benchmarks/eval_programming_bench.py bench_runs/programming_bench/<TIMESTAMP>
```

### Baseline vs Semiotic (what changes)

| Metric                            | Baseline (free text) | Semiotic (schema) |
|-----------------------------------|----------------------|-------------------|
| Requiredâ€‘field coverage           | ~0.55                | 1.00              |
| Severity present                  | 0%                   | 100%              |
| Extra keys rate (toolâ€‘calling)    | n/a                  | 0.0%              |
| Arg coverage (toolâ€‘calling)       | n/a                  | 1.00              |

Numbers above are from the included microâ€‘benchmarks (n=8) and the SambaNova functionâ€‘calling runs.

## ğŸš€ Key Results at a Glance

| Metric                       | Baseline (Free text) | TESC (Structured)        |
|-----------------------------|----------------------|--------------------------|
| Output form                  | Free text            | JSON (valid via `response_schema`) |
| Audit contract validity      | 0%                   | 100%                     |
| Mean field coverage (audit)  | 0.02                 | 1.00                     |
| Injectivity plateau (Î¸â‰¥0.98) | â€”                    | 1.0                      |
| Intra-config similarity      | â€”                    | 0.962                    |
| Programming coverage (CI)    | 0.53 [0.43,0.62]     | 0.88 [0.62,1.00]         |
| Repair pass rate (CI)        | 0.75 [0.41,0.93]     | 0.88 [0.53,0.98]         |

---

## ğŸ”¬ Methods Overview
- Semiotic Configuration (S = âŸ¨instruction, schema, markers, temperatureâŸ©)
- Generation via Gemini 2.5 Flash with enforced JSON schemas
- Evaluation with external embeddings (Qwen3 0.6B / ST) to avoid circularity
- Metrics: injectivity, uncertainty (Î”sÂ·Î”c), dynamics (dc/dt = f(s,c)+Î·), robustness

---

## ğŸ“ˆ Benchmarks (this repo)

### Programming Pitfalls (n=8 canonical cases)
- Mutable default args; Off-by-one; Resource leaks; Bare excepts; Shallow copy; SQL injection; Path traversal; Float equality
- TESC improves coverage & actionability (issues/risks/patch/tests/severity) and repair correctness (pass rates)

### Semiotic Uncertainty (pairwise; 4-level regimes)
- Pairwise Î”sÂ·Î”c against lab runs, and 4 regimes (ultra-precise/precise/vague/ultra-vague) for EN/ES prompts

### Dynamics (held-out)
- Linear fit of c_{t+1} from (c_t, s_t) with intra-config perturbations; report RÂ² per dimension, RMSE, deterministic fraction

---

## âš¡ Quickstart

1) Install dependencies (Poetry)
```
poetry install
```

2) Set your API key
```
# copy .env.example to .env and set GEMINI_API_KEY, or export it
export GEMINI_API_KEY="<your_key>"
```

3) Programming benchmarks
```
# Review (baseline vs TESC) â†’ results.json + CI table
poetry run python benchmarks/run_programming_bench.py
poetry run python benchmarks/eval_programming_bench.py bench_runs/programming_bench/<TIMESTAMP>
poetry run python benchmarks/make_ci_tables.py --prog-bench bench_runs/programming_bench/<TIMESTAMP>

# Repair (baseline vs TESC) â†’ repair_results.json + CI table
poetry run python benchmarks/run_programming_repair.py
poetry run python benchmarks/eval_programming_repair.py bench_runs/programming_repair/<TIMESTAMP>
poetry run python benchmarks/make_ci_tables.py --prog-repair bench_runs/programming_repair/<TIMESTAMP>
```

4) Uncertainty & Dynamics
```
# 4-level regimes (EN/ES)
poetry run python benchmarks/run_uncertainty_levels.py --per-level 6 --lang en
poetry run python benchmarks/run_uncertainty_levels.py --per-level 6 --lang es

# Pairwise uncertainty & dynamics on your lab run folder
poetry run python benchmarks/tesc_uncertainty_eval.py lab_runs/<RUN_ID>
poetry run python benchmarks/tesc_dynamics_eval.py lab_runs/<RUN_ID>
```

---

## ğŸ“¦ Repository Structure
```
.
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ data/programming_cases.jsonl
â”‚   â”œâ”€â”€ semio_utils.py                # reads GEMINI_API_KEY
â”‚   â”œâ”€â”€ sambanova_utils.py            # reads SAMBA_API_KEY (optional)
â”‚   â”œâ”€â”€ run_programming_bench.py      # review generation
â”‚   â”œâ”€â”€ eval_programming_bench.py     # review evaluation
â”‚   â”œâ”€â”€ run_programming_repair.py     # repair generation
â”‚   â”œâ”€â”€ eval_programming_repair.py    # repair evaluation
â”‚   â”œâ”€â”€ run_uncertainty_levels.py     # 4-level regimes (EN/ES)
â”‚   â”œâ”€â”€ tesc_uncertainty_eval.py      # pairwise Î”sÂ·Î”c on lab run
â”‚   â”œâ”€â”€ tesc_dynamics_eval.py         # dynamics fit on lab run
â”‚   â””â”€â”€ make_ci_tables.py             # CI LaTeX tables (optional)
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md (this)
```

---

## ğŸ“š Citation
If you use this work, please cite (preprint, technical report):
```bibtex
@techreport{rios2025tesc,
  title        = {TESC: Deterministic Cognitive State Control in LLMs via Structured Outputs and Semiotic Configuration},
  author       = {RÃ­os Montecinos, Daslav and RÃ­os Saldivar, Oscar},
  institution  = {AMAWTA Research},
  number       = {TESC-TR-2025-09},
  type         = {Technical Report},
  year         = {2025},
  month        = {September},
  url          = {https://github.com/Amawta-labs/TESC},
  note         = {Public code and benchmarks; preprint forthcoming}
}
```

## ğŸ§© License
Apache License 2.0. See [LICENSE](./LICENSE) for details.

## ğŸŒ AMAWTA Research
A father, a son, and an AI partner exploring deterministic cognition in LLMs.  
Independent lab Â· Santiago, Chile Â· 2025

## ğŸ™ Acknowledgments

We thank:
- [Thunder Compute](https://www.thundercompute.com) for oneâ€‘click GPU instances that accelerated our experiments.
- [SambaNova Systems](https://www.sambanova.ai) for an OpenAIâ€‘compatible functionâ€‘calling interface and developer support that enabled crossâ€‘model structuredâ€‘output validation.
- [OpenAI](https://openai.com) for tools and research infrastructure that supported parts of this work.

Trademarks and names remain the property of their respective owners. This acknowledgment does not imply endorsement or partnership.
# 5) (Optional) Use SambaNova models (function calling)
```
export SAMBA_API_KEY="<your_sambanova_key>"
# Optionally: export SAMBA_MODEL=Meta-Llama-3.1-70B-Instruct

# Programming review with SambaNova
poetry run python benchmarks/run_programming_bench_sambanova.py
# Evaluate with the same eval script
poetry run python benchmarks/eval_programming_bench.py bench_runs/programming_bench_samba/<TIMESTAMP>
```
